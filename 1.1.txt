Q1:Why log 2 as a quantification of information
Type: Abstraction
Future use:
    We require information meets:
        1.Monotonicity: The less probability, the more information
        2.Additivity: Its methmatical expression should meet: I(A,B)=I(A)+I(B).
    As if we know independent events A and B, the information contains both of their information.
    Assume that x = P(A), y = P(B), what we want should meet Cauchy's functional equation:
        f(x·y) = f(x) + f(y)
    Take the partial derivative of y：
        df(x.y)/dy ---> x·f'(x·y) = f(y)
    Let y = 1:
        xf'(x) = f(1) = k
    then: 
        f'(x) = k/x
    Integration:
        f(x) = kln(x) + C
    Check: 
        since x represents probability, so when x = 1, the information should be 0, thus C = 0. 
    That's why we choose log to describe information.
As for log2, since we use bit to quantify as how many yes or no requires to get to this probability

Q2:What do fractional bits mean
Type: Long term in average
Future use:
    Suppose we roll two dices,total probabilities are 36, and the information is I = log2(36/1) = 5.17...
    means if we only count one roll, we need 6 bits,if we count 10 rolls, what we need is 52 but not 60. 
    Fractional bits do not describe a single outcome’s code length, 
    but the asymptotic average bits per symbol under optimal prefix coding.

Q3:What does H(x) means
Type: Lower bound
Future use:
    H(x) is the entropy of x，is the expectation of I(x), 
    counts as the minimum average code length under the optimal prefix code 

Q4:Why can we call H(x) as entropy
Type: Abstraction
Future use:

Q5:The minimum average code quantifies the same range of minimum average code under the optimal prefix code
why we need to add the regulation: under the optimal prefix code
Type: Abstraction

Q6:Why should we use fixed length encoding when we don't know the probability
Type: Lower bound
    According to principle of maximum entropy, if we only know there are N symbols in total, to make sure we transmit the whole information, we need to assume   that it requires as much bit as it is, and that's when equal probability is, and fixed length encoding should be used.

Q7:What is two's complement? Why is it important? How to compute it?
Type: Abstraction
Future use: A unitive logic
    For a binary string, its two's complement is 1 plus its bitwise inversion, for example, for 1 , 0000 0001, its two's complement -1 is 1111 1111。
    Notice that only when the binary string represents an integer the rule is correct.
    Two's complement allows a unitive logic of add and minus, because A - B = A + ~B

Q8:What is the work field of parity check? How can we solve other errors?

If we want to detect E errors, we need minimum Hamming distance of E+1 between code words.

Q9:How can we ensure the Hamming distance within a case that N bits fixed length code and M messages?
Type:scope 
[Layer 1: General feasibility bounds]

Step 1: If M = 2^(N-k)
    → Treat as linear code
    → d_min = minimum weight of non-zero codeword

Step 2: Otherwise, use classical bounds to estimate d_min

    - Singleton Bound:
        d_min ≤ N - log2(M) + 1
        (General, loose)

    - Plotkin Bound:
        d_min <= NM/2(M-1)
        Useful when d_min is large (roughly d > N/2)

Q10：What does the worst case mean when we are discussing correction based on Hamming distance.
Type:Abstraction
    Worst case correction capability is determined by the minimum Hamming distance dmin, not by average distances.
    Notice that receiver would naturally use nearest neighbour decode, e.g.: receive: 00011, HD to 00000: 2, HD to 11111: 3, so receiver think it receives 0. 
    For example, when we are discussing on a 3 bits fixed-length code with 5 messages, their minimum hamming distance is 1, repeat 5 times, min HD = 5,
    correction = [(d_min-1)/2]    (must be integer)
    How can the correction formula stands? 
        In a 2 bit code case, it's like an axis, the information would be corrected to the closer end.






