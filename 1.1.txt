Q1:Why log 2 as a quantification of information
Type: Abstraction
Future use:
    We require information meets:
        1.Monotonicity: The less probability, the more information
        2.Additivity: Its methmatical expression should meet: I(A,B)=I(A)+I(B).
    As if we know independent events A and B, the information contains both of their information.
    Assume that x = P(A), y = P(B), what we want should meet Cauchy's functional equation:
        f(x·y) = f(x) + f(y)
    Take the partial derivative of y：
        df(x.y)/dy ---> x·f'(x·y) = f(y)
    Let y = 1:
        xf'(x) = f(1) = k
    then: 
        f'(x) = k/x
    Integration:
        f(x) = kln(x) + C
    Check: 
        since x represents probability, so when x = 1, the information should be 0, thus C = 0. 
    That's why we choose log to describe information.
As for log2, since we use bit to quantify as how many yes or no requires to get to this probability

Q2:What do fractional bits mean
Type: Long term in average
Future use:
    Suppose we roll two dices,total probabilities are 36, and the information is I = log2(36/1) = 5.17...
    means if we only count one roll, we need 6 bits,if we count 10 rolls, what we need is 52 but not 60. 
    Fractional bits do not describe a single outcome’s code length, 
    but the asymptotic average bits per symbol under optimal prefix coding.

Q3:What does H(x) means
Type: Lower bound
Future use:
    H(x) is the entropy of x，is the expectation of I(x), 
    counts as the minimum average code length under the optimal prefix code 

Q4:Why can we call H(x) as entropy
Type: Abstraction
Future use:

Q5:The minimum average code quantifies the same range of minimum average code under the optimal prefix code
why we need to add the regulation: under the optimal prefix code
Type: Abstraction

Q6:Why should we use fixed length encoding when we don't know the probability
Type: Lower bound
    According to principle of maximum entropy, if we only know there are N symbols in total, to make sure we transmit the whole information, we need to assume   that it requires as much bit as it is, and that's when equal probability is, and fixed length encoding should be used.

Q7:What is two's complement? Why is it important? How to compute it?
Type: Abstraction
Future use: A unitive logic
    For a binary string, its two's complement is 1 plus its bitwise inversion, for example, for 1 , 0000 0001, its two's complement -1 is 1111 1111。
    Notice that only when the binary string represents an integer the rule is correct.
    Two's complement allows a unitive logic of add and minus, because A - B = A + ~B

Q8:What is the effect of parity?
